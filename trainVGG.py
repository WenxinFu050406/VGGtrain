import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.models import vgg16, vgg19, vgg11
from torch.utils.data import DataLoader, random_split
from torchvision import transforms, datasets
from tqdm import tqdm
import time

# -------------------- é…ç½®å‚æ•° --------------------
BATCH_SIZE = 256
NUM_EPOCHS = 100
NUM_CLASSES = 10
CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']
VAL_RATIO = 0.2  # 20%è®­ç»ƒé›†ä½œä¸ºéªŒè¯é›†
FREEZE_LAYERS = 10  # å†»ç»“å‰10å±‚å·ç§¯å±‚
INIT_LR = 1e-4
MIN_LR = 1e-6

# -------------------- æ•°æ®å¢å¼º --------------------
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

eval_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# -------------------- æ•°æ®åŠ è½½ --------------------
print("âŒ› åŠ è½½CIFAR-10æ•°æ®é›†...")
train_val_dataset = datasets.CIFAR10(root='./data', train=True,
                                     download=True, transform=train_transform)
test_dataset = datasets.CIFAR10(root='./data', train=False,
                                download=True, transform=eval_transform)

# åˆ†å‰²è®­ç»ƒé›†ä¸ºè®­ç»ƒå’ŒéªŒè¯ (80:20)
train_size = int((1 - VAL_RATIO) * len(train_val_dataset))
val_size = len(train_val_dataset) - train_size
train_dataset, val_dataset = random_split(
    train_val_dataset, [train_size, val_size],
    generator=torch.Generator().manual_seed(42)
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                          shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,
                        shuffle=False, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,
                         shuffle=False, num_workers=4, pin_memory=True)

print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
print(f"è®­ç»ƒé›†: {len(train_dataset)}å¼ ")
print(f"éªŒè¯é›†: {len(val_dataset)}å¼ ")
print(f"æµ‹è¯•é›†: {len(test_dataset)}å¼ ")


# -------------------- ä¼˜åŒ–çš„VGGæ¨¡å‹ --------------------
class OptimizedVGG(nn.Module):
    def __init__(self, model_type):
        super().__init__()
        if model_type == 'vgg11':
            vgg = vgg11(pretrained=True)
        elif model_type == 'vgg16':
            vgg = vgg16(pretrained=True)
        elif model_type == 'vgg19':
            vgg = vgg19(pretrained=True)
        else:
            raise ValueError("Unsupported model type. Choose from 'vgg11', 'vgg16', or 'vgg19'.")

        # å†»ç»“æŒ‡å®šå±‚
        for idx, layer in enumerate(vgg.features.children()):
            if idx < FREEZE_LAYERS:
                for param in layer.parameters():
                    param.requires_grad = False

        self.features = vgg.features
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

        # ä¼˜åŒ–åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(1024, NUM_CLASSES)
        )

        # åˆå§‹åŒ–åˆ†ç±»å¤´æƒé‡
        for m in self.classifier:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


# -------------------- è®­ç»ƒå‡½æ•° --------------------
def train_model(model, train_loader, val_loader, device, model_name):
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=INIT_LR,
        weight_decay=1e-4
    )

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=3, verbose=True
    )

    criterion = nn.CrossEntropyLoss()
    best_val_acc = 0.0

    for epoch in range(NUM_EPOCHS):
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0

        # è®­ç»ƒé˜¶æ®µ
        for images, labels in tqdm(train_loader, desc=f"{model_name} Epoch {epoch + 1}/{NUM_EPOCHS}"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
            train_loss += loss.item()

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss /= len(train_loader)
        train_acc = 100 * correct_train / total_train

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)

        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(val_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f"{model_name}_best_model.pth")
            print(f"\nğŸ’¾ ä¿å­˜ {model_name} æœ€ä½³æ¨¡å‹ | éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # æ‰“å°æ—¥å¿—
        print(f"\n{model_name} Epoch {epoch + 1}/{NUM_EPOCHS} ç»“æœ:")
        print(f"è®­ç»ƒé›†: æŸå¤±={train_loss:.4f} | å‡†ç¡®ç‡={train_acc:.2f}%")
        print(f"éªŒè¯é›†: æŸå¤±={val_loss:.4f} | å‡†ç¡®ç‡={val_acc:.2f}%")
        print(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

    # è®­ç»ƒç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), f"{model_name}_final_model.pth")
    print(f"\nğŸ’¾ ä¿å­˜ {model_name} æœ€ç»ˆæ¨¡å‹")


# -------------------- è¯„ä¼°å‡½æ•° --------------------
def evaluate(model, data_loader, criterion, device):
    model.eval()
    loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss += criterion(outputs, labels).item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return loss / len(data_loader), 100 * correct / total


# -------------------- æµ‹è¯•å‡½æ•° --------------------
def test_model(model_path, test_loader, device, model_name="æ¨¡å‹"):
    model = OptimizedVGG(model_name.split("_")[0]).to(device)
    model.load_state_dict(torch.load(model_path))

    test_loss, test_acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), device)
    print(f"\nğŸ” {model_name}åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°:")
    print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f} | æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    return test_acc


# -------------------- ä¸»å‡½æ•° --------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        print("âœ… å·²å¯ç”¨cuDNNè‡ªåŠ¨ä¼˜åŒ–")

    model_types = [ 'vgg19']
    test_results = {}

    for model_type in model_types:
        # åˆå§‹åŒ–æ¨¡å‹
        print(f"\nğŸ”„ åˆå§‹åŒ– {model_type} æ¨¡å‹...")
        model = OptimizedVGG(model_type).to(device)

        # æ‰“å°å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°: {total_params:,} | å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ {model_type}")
        start_time = time.time()
        train_model(model, train_loader, val_loader, device, model_type)
        print(f"\nâ±ï¸ {model_type} æ€»è®­ç»ƒæ—¶é—´: {(time.time() - start_time) / 60:.1f}åˆ†é’Ÿ")

        # æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯• {model_type} é˜¶æ®µ...")
        best_acc = test_model(f"{model_type}_best_model.pth", test_loader, device, f"{model_type} æœ€ä½³æ¨¡å‹")
        final_acc = test_model(f"{model_type}_final_model.pth", test_loader, device, f"{model_type} æœ€ç»ˆæ¨¡å‹")

        test_results[model_type] = (best_acc, final_acc)

    # æ‰“å°æ‰€æœ‰æ¨¡å‹çš„æµ‹è¯•ç»“æœå¯¹æ¯”
    print("\nğŸ“Š æ‰€æœ‰æ¨¡å‹æµ‹è¯•ç»“æœå¯¹æ¯”:")
    for model_type, (best_acc, final_acc) in test_results.items():
        print(f"{model_type} æœ€ä½³æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"{model_type} æœ€ç»ˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open("test_results.txt", "w") as f:
        for model_type, (best_acc, final_acc) in test_results.items():
            f.write(f"{model_type} æœ€ä½³æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%\n")
            f.write(f"{model_type} æœ€ç»ˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%\n")

    print("\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆ!")


if __name__ == "__main__":
    main()
